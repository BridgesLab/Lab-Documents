---
title: "Examples of Bayesian Analyses"
author: "Dave Bridges"
date: "2024-08-12"
format: 
  html:
    toc: true
    toc-location: right
theme: journal
execute:
  keep-md: true
  message: false
  warning: false
---

## Some Common Analyses

As a general rule we do four analyses fairly commonly in the Bridges Lab, summarized here based on the nature of the independent and dependent variables:

| Dependent Variable        | Independent Variable      | Analysis                                    |
|------------------------|--------------------------|-----------------------|
| Continuous                | Continuous                | Linear Regression                           |
| Continuous                | Counts (Yes/No or Groups) | Binomial Regression                         |
| Counts (Yes/No or Groups) | Continuous                | Pairwise Test (*t*-test/Mann-Whitney/ANOVA) |
| Counts (Yes/No or Groups) | Counts (Yes/No or Groups) | $\chi^2$ or Fisher's Test                   |

Lets take a Bayesian approach to each of these using the Mtcars dataset, which looks like this after a bit of fiddling.

```{r mtcars}
library(quarto)
library(tinytex)
library(knitr)
library(dplyr)
mtcars.data <- 
  as.data.frame(mtcars) %>%
  mutate(transmission = factor(case_when(am==0~"automatic",
                           am==1~"manual")),
         engine=factor(case_when(vs==0~"V-Shaped",
                       vs==1~"Straight")),
         cylindars=factor(cyl))
  
kable(mtcars.data,caption="The mtcars dataset")
```

## What is a Pairwise Testing Equivalent?

Lets start by testing if there is a relationship between one quantitative variables, the mpg (miles per gallon) and a categorical variable - transmission (manual or automatic transmission) using the default priors

```{r brms-pairwise}
#| output: false
library(brms)
pairwise.fit <- brm(mpg~transmission,data=mtcars.data,
                    sample_prior = TRUE)
```

```{r pairwise-analysis}
library(broom.mixed)
tidy(pairwise.fit) %>% kable(caption="Summary of model fit for mpg versus transmission")

plot(pairwise.fit)
hypothesis(pairwise.fit, "transmissionmanual>0") # testing for whether manual tramsmission has higher mpg
```

As you can see, this analysis estimates that the manual transmission has a `r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Estimate` higher mpg ($\pm$ `r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Est.Error`) with a very high Bayes Factor (`r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Evid.Ratio`) and posterior probability (`r hypothesis(pairwise.fit, "transmissionmanual>0")$hypothesis$Post.Prob`).

Looking at the model diagnostics three things are relevant:

* Was there convergence in the model.  This is measured by Rhat (also known as potential scale reduction factor (PSRF), or the Gelman-Rubin statistic).  It should be between 0.99 and 1.01.
* What was the ESS (effective sample size, not a great name, but a measure of the number of samples of the posterior distribution).  Generally, an $ESS > 400$ is considered good for reliable estimates.  This is most important for the random effects rather than the fixed effects.

Lets take a look at these:

```{r pairwise-diagnostics}
kable(data.frame(
  Parameter = names(rhat(pairwise.fit)),
  Rhat = format(rhat(pairwise.fit), nsmall = 5)),
caption="Rhat values for model testing association between mpg and transmission (should b between 0.99 and 1.01 for convergence",
row.names = F)

tidy(pairwise.fit, effects = "ran_pars", ess = TRUE) %>% kable(caption="Model random effects for mpg vs transmission")
```

The posterior distribution for the effect of a manual transmission is here:

```{r pairwise-posterior}
library(ggplot2)
as_draws_df(pairwise.fit) %>%
  ggplot(aes(x=b_transmissionmanual)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Effect of manual transmission (mpg)",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

We didnt specify the model type (gausian is the default). We also used the default priors here, which were

```{r pairwise-priors}
prior_summary(pairwise.fit) %>% kable(caption="Default priors for a pairwise analysis of mpg vs transmission in the mtcars data")
```

This includes flat priors for the beta coefficients, student's *t* distributions for intercept (centered around the mean mpg) and residual error (centered around zero).  

### What if I Want an ANOVA

Perhaps I only am interested in whether there is a difference between groups, but not pairwise differences (*i.e* an ANOVA).  This can be done as well (again using default priors).

```{r anova-example}
#| output: false
anova.fit <- brm(mpg~0+cylindars,data=mtcars.data, sample_prior = TRUE) #zero sets there to be no intercept, shows the mean for each group
anova.fit.null <- brm(mpg~0,data=mtcars.data, sample_prior = TRUE) #null model

#estimate the bayes factor
bayes_factor(anova.fit,anova.fit.null) -> anova.bf
post_prob(anova.fit,anova.fit.null) -> anova.pp
```

Now lets look at the results of those analyses comparasons.  The bayes factor for the ANOVA is `r anova.bf$bf` which is very extreme evidence for a difference between groups.  The posterior probabilities are `r anova.pp[1]` (very hight) for the fitted model and `r anova.pp[2]` (very low) for the null model.


```{r anova-results}
prior_summary(anova.fit) %>% kable(caption="Summary of priors for comparason between cylindar numbers on mpg")

#model fitting parameters
kable(data.frame(
  Parameter = names(rhat(anova.fit)),
  Rhat = format(rhat(anova.fit), nsmall = 5)),
caption="Rhat values for model testing association between mpg and cylindars (should b between 0.99 and 1.01 for convergence",
row.names = F)

tidy(anova.fit, effects = "ran_pars", ess = TRUE) %>% kable(caption="Model random effects for mpg vs cylindars")

#create a teable of the pairwise hypotheses
rbind(hypothesis(anova.fit, "cylindars4>cylindars6")$hypothesis,
      hypothesis(anova.fit, "cylindars4>cylindars8")$hypothesis,
      hypothesis(anova.fit, "cylindars6>cylindars8")$hypothesis) %>%
  kable(caption="Pairwise hypothesis tests for cylindars on mpg")


tidy(anova.fit) %>% kable(caption="Summary of model fit for mpg versus cylindars")
plot(anova.fit)

library(tidyr) #for pivot wider
as_draws_df(anova.fit) %>%
  select(starts_with('b')) %>%
  pivot_longer(cols=everything(),
             names_to = "term",
             values_to="mpg") %>%
  ggplot(aes(x=mpg,fill=term)) +
  geom_density() +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
     x="Miles Per Gallon",
     title="Posterior probabilities") +
  scale_fill_discrete(name="") +
  theme_classic(base_size=16) +
  theme(legend.position="top")
```

## What is a Linear Regression Equivalent?

Lets start by testing if there is a relationship between two quantitative variables, the mpg (miles per gallon) and the weight (wt) using the default priors

```{r brms-linear-regression}
#| output: false
linear.fit <- brm(mpg~wt,data=mtcars,
                  sample_prior = TRUE)
```

Lets see what this looks like

```{r linear-analysis}
prior_summary(linear.fit) %>% kable(caption="Prior summary for effects of weight on mpg")
tidy(linear.fit) %>% kable(caption="Summary of model fit for mpg versus weight")

plot(linear.fit)
hypothesis(linear.fit, "wt<0") # testing for whether weight has a negative effect on mpg

as_draws_df(linear.fit) %>%
  ggplot(aes(x=b_wt)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Effect of Weight (mpg/1000 kg)",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

Notice that the priors were the same here (they are based on the dependent variable), and again there is a highly probable inverse relationship between mpg and weight (heavier cars get lower fuel economy).

### What if I am more interested in the R-squared?

To do this we can use the `bayes_R2` function.

```{r r2-calculations}
kable(bayes_R2(linear.fit),caption="Estimates for R2 between weight and mpg")
r2.probs <- bayes_R2(linear.fit, summary=F) #summary false is to get the posterior probabilities
ggplot(data=r2.probs, aes(x=R2)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="R2",
       title="Posterior probabilities") +
  lims(x=c(0,1)) +
  theme_classic(base_size=16)
```

## What is a Chi-Squared Equivalent

First lets do an example with a standard $\chi^2$ test (and a Fisher's test since the counts are quite low) on whether there is a relationship bewteen engine type and transmission type.

```{r brms-chisq-data}
library(tidyr) #for pivot wider
library(tibble) #for column to rowname
engine.trans.counts <-
  mtcars.data %>%
  group_by(engine,transmission) %>%
  count() %>%
  pivot_wider(names_from=transmission,values_from=n) %>%
  column_to_rownames('engine')

chisq.test(engine.trans.counts) %>% tidy %>% kable(caption="Chi-squared test for engine/transmission")
fisher.test(engine.trans.counts) %>% tidy %>% kable(caption="Fisher's test for engine/transmission")
```

Both agree, not much of a relationship here. For the brms modelling we need to make a few changes. First, we will use a bernoulli distribution since our data is only zeros and ones, again we will use all the default priors.

```{r brms-chisq}
#| output: false
# Fit the model
counts.model <- brm(engine ~ transmission, 
           data = mtcars.data, 
           family = bernoulli())
```

Lets look at these results

```{r chisq-analysis}
prior_summary(counts.model) %>% kable(caption="Prior summary for effects of transmission on engine type")
tidy(counts.model) %>% kable(caption="Summary of model fit for transmission versus engine type")

plot(counts.model)
hypothesis(counts.model, "transmissionmanual<0") # testing for whether weight has a negative effect on mpg

as_draws_df(counts.model) %>%
  ggplot(aes(x=b_transmissionmanual)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Beta coefficient for manual transmission",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

Now in this case there is moderate evidence for a negative relationship between transmnission and engine type ($\beta$=`r fixef(counts.model)['transmissionmanual',"Estimate"]` $\pm$ `r fixef(counts.model)['transmissionmanual',"Est.Error"]`) with a Bayes Factor of `r hypothesis(counts.model, "transmissionmanual<0")$hypothesis$Evid.Ratio` and a Posterior Probability of `r hypothesis(counts.model, "transmissionmanual<0")$hypothesis$Post.Prob`.

## What is a Binomial Regression Equivalent?

Lets now look at the relationsbip between transmission type (binomial variable) and weight (continuous variable). Again we will use a bernouli distribution (which will use, by default a logit link function)

```{r brms-binomial}
#| output: false
# Fit the model
binomial.fit <- brm(transmission ~ wt, 
           data = mtcars.data, 
           family = bernoulli())
```

```{r binomial-analysis}
prior_summary(binomial.fit) %>% kable(caption="Prior summary for effects of transmission on engine type")

tidy(binomial.fit) %>% kable(caption="Summary of model fit for transmission versus engine type")

plot(binomial.fit)
hypothesis(binomial.fit, "wt<0") # testing for whether weight has a negative effect on mpg

as_draws_df(binomial.fit) %>%
  ggplot(aes(x=b_wt)) +
  geom_density(fill="blue") +
  geom_vline(xintercept=0,color="red",lty=2) +
  labs(y="",
       x="Beta coefficient for weight on transmission",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

Again there is strong evidence that a higher weight makes an automatic transmission more likely.

Hopefully these examples helped, but a couple things since we used defaults.

Think closely about your priors, if you have a good reason to set them to something other than the default you should. In this case the defaults worked pretty well and as you can see are set based on the data that is input. These are generally very **weakly informative priors** and the modelling could be improved on by setting your own.

Also remember, nowhere in here was there a p-value. We could consider a posterior probability of \>0.95 our cutoff for significance if we prefer, but make sure to state this in your methods (along with your choice of priors and the package used.)

Note this script used some examples generated by [perplexity.ai](https://www.perplexity.ai/) and then modified further

# Session Info

```{r sessionInfo}
sessionInfo()
```
