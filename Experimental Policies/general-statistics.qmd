---
title: "General Statistics"
author: "Dave Bridges"
date: "May 9, 2014"
format: 
  html:
    toc: true
    toc-location: right
    keep-md: true
    code-fold: true
    code-summary: "Show the code"
theme: journal
bibliography: references.bib   
execute: 
  message: false
---

```{r global_options}
library(knitr)
#figures makde will go to directory called figures, will make them as both png and pdf files 
opts_chunk$set(fig.path='figures/',dev=c('png','pdf'))
options(scipen = 2, digits = 3)
# set echo and message to TRUE if you want to display code blocks and code output respectively

knitr::knit_hooks$set(inline = function(x) {
  knitr:::format_sci(x, 'md')
})


superpose.eb <- function (x, y, ebl, ebu = ebl, length = 0.08, ...)
  arrows(x, y + ebu, x, y - ebl, angle = 90, code = 3,
  length = length, ...)

  
se <- function(x) sd(x, na.rm=T)/sqrt(length(x))

#load these packages, nearly always needed
library(tidyr)
library(dplyr)

# sets maize and blue color scheme
color.scheme <- c('#00274c', '#ffcb05')
```

# General Statistical Methods

There are several important concepts that we will adhere to in our group.  These involve design considerations, execution considerations and analysis concerns.  The standard for our field is null hypothesis significance testing, which means that we are generally comparing our data to a null hypothesis, generating an **effect size** and a **p-value**.  As a general rule, we report both of these both within our Rmd/qmd scripts, and in our publications.

We generally use an $\alpha$ of $p<0.05$ to determine significance, which means that (if true) we are rejecting the null hypothesis.  This is known as null hypothesis significance testing.

An alternative approach is to use a Bayesian approach, described in more detail in [this document](https://bridgeslab.github.io/Lab-Documents/Experimental%20Policies/bayesian-analyses.html)

# Pairwise Testing

If you have two groups (and two groups only) that you want to know if they are different, you will normally want to do a pairwise test.  This is **not** the case if you have paired data (before and after for example).  The most common of these is something called a Student's *t*-test, but this test has two key assumptions:

* The data are normally distributed
* The two groups have equal variance

## Testing the Assumptions

Best practice is to first test for normality, and if that test passes, to then test for equal variance

### Testing Normality

To test normality, we use a Shapiro-Wilk test (details on [Wikipedia](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) on each of your two groups).  Below is an example where there are two groups:

```{r normality}
#create seed for reproducibility
set.seed(1265)
test.data <- tibble(Treatment=c(rep("Experiment",6), rep("Control",6)),
           Result = rnorm(n=12, mean=10, sd=3))
#test.data$Treatment <- as.factor(test.data$Treatment)
kable(test.data, caption="The test data used in the following examples")
```

Each of the two groups, in this case **Test** and **Control** must have Shapiro-Wilk tests done separately.  Some sample code for this is below (requires dplyr to be loaded):

```{r normality-tests}
#filter only for the control data
control.data <- filter(test.data, Treatment=="Control")
#The broom package makes the results of the test appear in a table, with the tidy command
library(broom)

#run the Shapiro-Wilk test on the values
shapiro.test(control.data$Result) %>% tidy %>% kable(caption="Shapiro-Wilk test for normality of control data")

experiment.data <- filter(test.data, Treatment=="Experiment")
shapiro.test(test.data$Result) %>% tidy %>% kable(caption="Shapiro-Wilk test for normality of the test data")
```

Based on these results, since both p-values are >0.05 we do not reject the presumption of normality and can go on.  If one or more of the p-values were less than 0.05 we would then use a Mann-Whitney test (also known as a Wilcoxon rank sum test) will be done, see below for more details.

### Testing for Equal Variance

We generally use the [car](https://cran.r-project.org/web/packages/car/index.html) package which contains code for [Levene's Test](https://en.wikipedia.org/wiki/Levene%27s_test) to see if two groups can be assumed to have equal variance.  For more details see @car:

```{r levene-test}
#load the car package
library(car)

#runs the test, grouping by the Treatment variable
leveneTest(Result ~ Treatment, data=test.data) %>% tidy %>% kable(caption="Levene's test on test data")
```

## Performing the Appropriate Pairwise Test

The logic to follow is:

* If the Shapiro-Wilk test passes, do Levene's test.  If it fails for either group, move on to a **Wilcoxon Rank Sum Test**.
* If Levene's test *passes*, do a Student's *t* Test, which assumes equal variance.
* If Levene's test *fails*, do a Welch's *t* Test, which does not assume equal variance.

### Student's *t* Test

```{r student}
#The default for t.test in R is Welch's, so you need to set the var.equal variable to be TRUE
t.test(Result~Treatment,data=test.data, var.equal=T) %>% tidy %>% kable(caption="Student's t test for test data")
```

### Welch's *t* Test

```{r welch}
#The default for t.test in R is Welch's, so you need to set the var.equal variable to be FALSE, or leave the default
t.test(Result~Treatment,data=test.data, var.equal=F) %>% tidy %>% kable(caption="Welch's t test for test data")
```

### Wilcoxon Rank Sum Test

```{r wilcoxon}
# no need to specify anything about variance
wilcox.test(Result~Treatment,data=test.data) %>% tidy %>% kable(caption="Mann-Whitney test for test data")
```

# Corrections for Multiple Observations

The best illustration I have seen for the need for multiple observation corrections is this cartoon from XKCD (see http://xkcd.com/882/):

![Significance by XKCD.  Image is from http://imgs.xkcd.com/comics/significant.png](http://imgs.xkcd.com/comics/significant.png "Significance by XKCD.  Image is from http://imgs.xkcd.com/comics/significant.png")


Any conceptually coherent set of observations must therefore be corrected for multiple observations.  In most cases, we will use the method of @Benjamini1995 since our p-values are not entirely independent.  Some sample code for this is here:

```{r multiple-observations}
p.values <- c(0.023, 0.043, 0.056, 0.421, 0.012)
data.frame(unadjusted = p.values, adjusted=p.adjust(p.values, method="BH")) %>% 
  kable(caption="Effects of adjusting p-values by the method of Benjamini-Hochberg")
```

# Session Information

```{r session-information}
sessionInfo()
```

# References


::: {#refs}
:::