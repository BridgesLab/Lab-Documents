---
title: "Bayesian Statistical Approaches"
author: "Dave Bridges"
date: "2024-07-31"
format: html
theme: journal
execute:
  keep-md: true
editor: visual
---

While we most often use classical frequentist statistical approaches, the norm in the molecular physiology field, I have been thinking a lot about Bayesian approaches, especially from a public health and nutrition perspective.  In these fields the data tend to be less clear and I find myself updating my opinions often based on new data.  

### How Much Protein is Optimal Post-Exercise

One example is the question of how much protein is optimal post-resistance training workout.  I had been teaching for years that the max was 20-30g and beyond that there wsa no advantage.  This was based on several feeding experiments with whey protein looking at muscle protein synthesis as the endpoint.  However in December a provocative paper came out showing that up to 100g could be absorbed and stored and that the true maximum may be higher.  There are several important differences in this study including a more natural protein source (milk proteins mostly a combination of whey and casein, compared with casein alone) and much more rigorous endpoints by the use of stable isotopes.  I am a cynical person by nature, but this paper made me change my opinion greatly.  In other words by prior hypothesis (max protein uptake is 20-30g) was updated by new data (this new paper) and my new posterior opinion suggests the levels might be higher.  Fundamentally this happens to me a lot when newer (or better) data updates our understanding of the world and we update the likliehood of something being true.  This is an example of inferential Bayesian reasoning.  The math behind this is this (from https://en.wikipedia.org/wiki/Bayesian_inference):


* $P(H|E)$ is the probability of hypothesis A given that data B was obtained.  Also known as the *posterior probability*.
* $P(E|H)$ is the probability of observing E given this hypothesis
* $P(H)$ is the probability of the hypothesis before hte data (the *prior probability*)
* $P(E)$ is the probability of the evidence, or the *marginal liklihood*

Together these are connected by Bayes' rule

$$P(H|E) = \frac{P(E|H)P(H)}{P(E)} $$
In other words my prior hypothesis (max protein post workout is 20-30g; $P(H)$ was updated by this new data (represented by $\frac{P(E|H)}{P(E)}$) to give me an updated posterior probability of that hypothesis being true given this evidence ($P(H|E)$).  Roughly I would say i had about 70% certainty that 20-30g was the maximum before the study but now only about ~10% certainty after reading that study, so re-arranging we would get:

$$
0.1 = \frac{P(E|H)}{P(E)} \times 0.7
$$
$$
\frac{P(E|H)}{P(E)} = 0.1/0.7 = 0.07
$$

0.07 is much less than one, so based on my rough estimate the study changed my opinion by $1/0.07=~14$ fold.  Another way to think about this is to compare the **posterior** probabilities of two hypotheses or models ($BF=\frac{Pr(E|H_1)}{Pr(E|H_2)}$) a value which is known as the Bayes Factor.  This is because the $P(E)$ term is not expected to change depending on the hypothesis, because it is the overall probability of the observed data.

### Interpreting a Bayesian Analysis

Note that now we are reporting a poster probability ($P(H|E)$) or a bayesian factor.  In the former case we are saying that based on our prior probability and our baysian factor we report the probability that a hypothesis is true given the evidence (so closer to 1 is better).  In the latter, if we report a Bayes Factor we are reporting how much we expect that these data should modify any prior probability ($P(H)$).  This is an important distinction because a Bayes Factor does not make any claims about what the scientist initially thought about how likely a hypothesis was, so is more generalizabl.  Maybe I had some reason to think that the probability that protein intake max was 20-30g was 70% but another scientist thought it was closer to 90%, we would get different posterior probabilities but the same BF (0.07):

$$P(H|E) = \frac{P(E|H)P(H)}{P(E)} = 0.07 \times 0.7=0.049$$
$$P(H|E) = \frac{P(E|H)P(H)}{P(E)} = 0.07 \times 0.9=0.063$$

## How to perform Bayesian Analyses

There

```{r brms}
library(brms)

fit1 <- brm(count ~ zAge + zBase * Trt + (1|patient),
            data = epilepsy, family = poisson())

summary(fit1)
plot(fit1, variable = c("b_Trt1", "b_zBase"))
plot(conditional_effects(fit1, effects = "zBase:Trt"))

library(dplyr)
epilepsy2 <-
  epilepsy %>%
  mutate(High.Age = zAge>median(zAge))
fit <- brm(
  count ~ High.Age,
  data = epilepsy2, 
  #family = poisson(),
  cores = 4,
  sample_prior = TRUE  # Important for hypothesis testing
)

hypothesis(fit, "High.AgeTRUE > 0")

posterior_samples <- as_draws_df(fit)

library(ggplot2)

ggplot(posterior_samples, aes(x = b_High.AgeTRUE)) +
  geom_density(fill = "skyblue", alpha = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Posterior Distribution of the Group Difference",
       x = "Difference between groups",
       y = "Density") +
  theme_classic(base_size=16)

library(bayesplot)

mcmc_areas(posterior_samples, pars = c("b_Intercept", "b_High.AgeTRUE"),
           prob = 0.95) +
  labs(title = "Posterior Distributions with 95% Credible Intervals")
```