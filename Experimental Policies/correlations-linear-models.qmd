---
title: "Correlations and Linear Models"
author: "Dave Bridges"
date: "2025-02-10"
editor: source
format: 
  html:
    toc: true
    toc-location: right
    keep-md: true
    code-fold: false
    code-summary: "Show the code"
  gfm:
    html-math-method: webtex
theme: journal
execute:
  echo: true
  warning: false
---

Lets say you want to explore the relationship between two continuous variables.  Here we will cover how to explore, graph, and quantify these relationships.

```{r dataset}
# sets maize and blue color scheme
color_scheme <- c("#00274c", "#ffcb05")

library(tidyverse)
library(moderndive)
```

We will use the `houseprices` dataset in the moderndive package.  It has data about a variety of factors about housing.  Here we will explore how the square feet of living space (`sqft_living`) relates to the `price`.

## Visualizing a Relationship

The best first thing to do is to make a plot looking at the two variables.  Here we will use ggplot, defining the x and y axis aesthetics, and adding both a linear and smoothed line (geom)

```{r price-sqft-living}
library(ggplot2)
ggplot(house_prices,
       aes(y=price,x=sqft_living)) +
  geom_point(alpha=0.1) + #transparency of the points
  geom_smooth(method="loess",color=color_scheme[2]) + #smoothed line
  geom_smooth(method="lm",color=color_scheme[1]) + #linear relationship
  theme_classic(base_size=16) #classic theme, and increasing the font size
```
This appears to be an approximately linear relationship (blue line), though the smooth best fit curve (maize line) suggests it might be nonlinear and slightly upward sloping.  For now lets focus on describing the linear relationship, we will come back to a non-linear relationship in [Non-Linear Relationships](#non_linear).

## Describing the Linear Relationship

In general there are two components to a relationship

1. The slope estimate and its error, in this example how much does price increase per square foot of living space
2. The linearity ($R^2$), or how much of the price is explained by square feet of living space.

In both cases this can be solved with a linear model, and the results visualized with the `tidy` and `glance` functions from the `broom` package

```{r linear-model}
lm.1 <- lm(price~sqft_living,data=house_prices)
library(broom) #for glance/tidy
library(knitr) #for kable
tidy(lm.1) %>% kable(caption="Estimates for the price/square foot relationship")
glance(lm.1) %>% kable(caption="Model fit for the price/square food relationship")
```

As you can see from these tables, we estimate that the price increases by \$`r lm.1$coefficients['sqft_living']` $\pm$ `r summary(lm.1)$coefficients['sqft_living', "Std. Error"]` per square foot of living space.  This is a highly significant relationship (p=`r summary(lm.1)$coefficients['sqft_living', "Pr(>|t|)"]`).  The $R^2$ for this model is estimated at `r summary(lm.1)$r.squared`.

### Testing the Assumptions of the Model

For a linear model there are four assumptions

* **Linearity**: The relationship between the predictor (x) and the outcome (y) is assumed to be linear. This means that for one unit change in the predictor, there is a constant change in the response.
* **Independence**: The residuals (error terms) are assumed to be independent of each other. This means we cannot predict the size of a residual from another residual or from the value of a predictor variable.
* **Homoscedasticity**: The residuals are assumed to have constant variance across all levels of the predictor variable. This is also known as homogeneity of variance.
* **Normality**: The residuals are assumed to be normally distributed. While perfect normality is not required, it suggests that the model is doing a good job of predicting the response.

These can be generated in base r or using the `lindia` package.  I'll show code for both, but we will focus on the four main diagnostic plots in base R.

```{r lm-assumptions}
par(mfrow = c(2, 2))
plot(lm.1)
#alternatively can use diagnostics in the lindia package
#library(lindia)
#gg_diagnose(lm.1)
```

#### Interpreting these Model Plots

* **Residuals vs. Fitted Plot** This plot helps detect non-linearity, unequal error variances, and outliers.  *Linearity*: The residuals should "bounce randomly" around the horizontal line at y=0. If you see a pattern, it suggests non-linearity.  *Equal Variance*: The vertical spread of residuals should be similar across the plot. A "horizontal band" indicates equal error variances. *Outliers*: Look for any points that stand out from the overall pattern.
* **Normal Q-Q Plot** This plot checks if residuals are normally distributed. *Normal Distribution*: Points should fall approximately along the diagonal line. Significant deviations suggest non-normality. *Tails*: Pay attention to deviations at the ends of the line, which indicate issues with the tails of the distribution.
* **Scale-Location Plot** This plot checks the assumption of equal variance (homoscedasticity). *Equal Variance*: Look for a horizontal line with randomly spread points. Any patterns or funneling suggest heteroscedasticity.
* **Residuals vs. Leverage Plot** This plot helps identify influential observations. *Influential Points*: Look for any points falling outside Cook's distance (usually marked by dashed lines). These are considered influential observations. *Leverage*: Points far to the right have high leverage, meaning they have a strong influence on the model coefficients.

My interpretation is that there are several issues with our model fit.  First from the residuals vs fitted plot, I see a cone-shape of points rather than a slug shape.  This means that as the price increases the error is worse.  Similarly the scale-location plot has an upward slope indicating the same thing. This suggests unequal variance across the estimates or *heteroscedasticity*.  In terms of normality, the Q-Q plot shows points lifting off the line, so this suggests these data are not *normally distributed*.  This can be confirmed by a Shapiro-Wilk test of either covariate or the model fit residuals:

```{r shapiro-test}
#note shapiro tests require 5000 or less values, so i randomly sampled
bind_rows(
sample(house_prices$price,5000,replace=F) %>% shapiro.test %>% tidy() %>% mutate(input="prices"), 
sample(house_prices$sqft_living,5000,replace=F) %>% shapiro.test %>% tidy() %>% mutate(input="sqft_living"), 
sample(residuals(lm.1),5000,replace=F) %>% shapiro.test %>% tidy() %>% mutate(input="redisuals")) %>% relocate(input) %>%
  kable(caption="Shapiro-Wilk Tests for Variables and Residuals",digits=c(0,3,99))
```

#### What to Do if Assumptions are Not Met

* **Linearity**.  Apply a non-linear transformation to the predictor or response variable (*e.g.*, log, square root, or polynomial terms).  Add interaction terms or higher-order terms (*e.g.*, quadratic) to the model to account for non-linear relationships.  Alternatively use a generalized additive model (GAM) or other non-linear modeling techniques.
* **Independence**  Check for omitted variables that might explain the correlation and include them in the model.  For time-series data, use models like ARIMA or include lagged variables to account for autocorrelation.  If independence is violated due to clustering or grouping, consider using mixed-effects models or hierarchical models.
* **Homoscedasticity (Constant Variance)**  Apply a log transformation or other variance-stabilizing transformations to the response variable.  Use weighted least squares (WLS) regression, which assigns weights inversely proportional to the variance of residuals.  Consider robust regression methods, such as those implemented in the robustbase package (lmrob).
* **Normality of Residuals**  Apply transformations (*e.g.*, log, square root) to the response variable if it is skewed.  Use non-parametric regression methods that do not assume normality, such as quantile regression.  If normality is violated due to outliers, investigate and either remove or down-weight their influence using robust regression techniques like least trimmed squares or MM-estimators.

## Bayesian Approach

Lets say you want to take a Bayesian approach to this question, and dispense with null hypothesis significance testing (see [here for an introduction](https://bridgeslab.github.io/Lab-Documents/Experimental%20Policies/bayesian-analyses.html)).  In this approach you would set a prior probablity, indexing what you would expect from this relationship then use a MCMC sampling method to estimate the $\beta$ coefficient and $R^2$ along with confidence intervals.  You can then derive a Bayes Factor and Posterior Probability of some hypothesis.  We will use the `brms` package for this.

For our prior probability, I have a general sense of this relationship.  I could assign an uninformative flat prior (meaning the $\beta$ coefficient can be any value) but I did a quick AI query and came up with a plausible global estimate of \$1663/sq ft with a standard deviation of \$602.  Im also setting the family to Student's *t* distribution which is more robust to outliers due to its heavier tails (normal or gaussian distribution of residualsis the default).

```{r brms-linear-regression}
#| output: false
library(brms)
housing.priors <- prior(normal(1663,602), class=b, coef=sqft_living)
brm.1 <- brm(price~sqft_living,
                  data=house_prices,
                  prior = housing.priors,
                  family = student(),
                  sample_prior = TRUE)
```

In summary, this is what we did:

```{r linear-analysis}
prior_summary(brm.1) %>% kable(caption="Prior summary for effects of square feet of living space on price")
library(broom.mixed)
tidy(brm.1) %>% kable(caption="Summary of model fit for price vs square feet of living space")

plot(brm.1)
hypothesis(brm.1, "sqft_living>0") # hypothesis testing for a positive relationship
hypothesis(brm.1, "sqft_living=1663") # hypothesis testing that my prior is correct

as_draws_df(brm.1) %>%
  ggplot(aes(x=b_sqft_living)) +
  geom_density(fill=color_scheme[2]) +
  #geom_vline(xintercept=0,color=color_scheme[2],lty=2) +
  labs(y="",
       x="Living Space (square feet)",
       title="Posterior probabilities") +
  theme_classic(base_size=16)
```

This gives me an estimate of \$`r fixef(brm.1)['sqft_living','Estimate']` $\pm$ `r fixef(brm.1)['sqft_living','Est.Error']`, compared to the linear model approach which estimated `r lm.1$coefficients['sqft_living']` $\pm$ `r summary(lm.1)$coefficients['sqft_living', "Std. Error"]` per square foot of living space, so very similar results. We also get a very similar value for $R^2$

```{r r2-calculations}
kable(bayes_R2(brm.1),caption="Estimates for R2 between price and square feet of living space")
r2.probs <- bayes_R2(brm.1, summary=F) #summary false is to get the posterior probabilities
ggplot(data=r2.probs, aes(x=R2)) +
  geom_density(fill=color_scheme[2]) +
  #geom_vline(xintercept=0,color=color_scheme[1],lty=2) +
  labs(y="",
       x="R2",
       title="Posterior probabilities") +
  #lims(x=c(0,1)) +
  theme_classic(base_size=16)
```
This approach has several advantages, including allowing for more robust handling of outliers and heteroscedasticity (due to using the Student's *t* distribution).  This allows for more robust posterior predictions.  This is probably why the estimates of the relationship are somewhat attenuated (because there are several influential high price/high square foot data points).

The linearity concerns may still remain in this approach.  We can evaluate this with a posterior probability check.  It shows again that when plotting the observed ($y$) and the predicted ($y_{rep}$) values, they tend to lift off the line as $y$ increases, so the relationship is nonlinear and slightly exponential in nature.

```{r brms-pp-check}
pp_check(brm.1, type = "scatter_avg")
pp_check(brm.1, type = "dens_overlay")
```
## Evaluating Non-Linear Relationships {#non_linear}

TODO

## Session Information

This script used [perplexity.ai](perplexity.ai) to help with its design and interpretation.

```{r session-information}
sessionInfo()
```
